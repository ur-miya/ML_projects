{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7xtrm2tMDsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the necessary modules and import the libraries"
      ],
      "metadata": {
        "id": "WQzy3z4x15_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZITucdxu1a7P",
        "outputId": "3245aa9f-d6de-4e58-ece4-32d6c45271b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: pywsd in /usr/local/lib/python3.12/dist-packages (1.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pywsd) (2.2.2)\n",
            "Requirement already satisfied: wn==0.0.23 in /usr/local/lib/python3.12/dist-packages (from pywsd) (0.0.23)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pywsd) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pywsd) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pywsd) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pywsd) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk gensim pywsd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "0B4zstYB3LGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the necessary resources for the nltk library"
      ],
      "metadata": {
        "id": "vopOVXh-2f3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9d1m0dv3Py9",
        "outputId": "6b0890eb-a633-4820-ff62-72d814657759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the test of the book\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "# Pruning official information\n",
        "start_match = re.search(r'CHAPTER I\\s*\\n\\s*[A-Z]', text)\n",
        "if start_match:\n",
        "    start = start_match.start()\n",
        "else:\n",
        "    start = text.find(\"CHAPTER I\")\n",
        "end = text.find(\"THE END\")\n",
        "text = text[start:end]\n",
        "# Dividing it into chapters\n",
        "chapters = re.split(r'CHAPTER [IVXLCDM]+', text)[13:]"
      ],
      "metadata": {
        "id": "qnoti3o-3P5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the text: reducing it to lowercase, removing stop words and applying lemmatization."
      ],
      "metadata": {
        "id": "r2A-25U33f5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "    return ' '.join(words)\n",
        "\n",
        "processed_chapters = [preprocess(chapter) for chapter in chapters]"
      ],
      "metadata": {
        "id": "aHb5eFme7J_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding 10 most important words for each chapter usinf TF-IDF matrix\n",
        "\n",
        "Then generating a potential title for each Chapter"
      ],
      "metadata": {
        "id": "zijq1Ldd4XK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def thematic_tfidf_analysis(chapters):\n",
        "\n",
        "    carroll_stop_words = ['alice', 'said', 'little', 'quite', 'rather', 'always', 'never', 'something', 'anything', 'everything', 'nothing']\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=20,\n",
        "        stop_words=carroll_stop_words,\n",
        "        min_df=1,\n",
        "        max_df=0.6,\n",
        "        ngram_range=(1, 3)\n",
        "    )\n",
        "    tfidf_matrix = vectorizer.fit_transform(processed_chapters)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    chapter_keywords = []\n",
        "    for i, chapter in enumerate(tfidf_matrix):\n",
        "        scores = chapter.toarray().flatten()\n",
        "        weighted_scores = []\n",
        "        for j, score in enumerate(scores):\n",
        "            word = feature_names[j]\n",
        "            length_bonus = 1.2 if len(word) > 5 else 1.0\n",
        "            if word in ['thought', 'began', 'turned', 'seemed']:\n",
        "                length_bonus *= 0.8\n",
        "            weighted_scores.append(score * length_bonus)\n",
        "        top_keywords = [feature_names[j] for j in np.array(weighted_scores).argsort()[-10:][::-1]]\n",
        "        chapter_keywords.append(top_keywords)\n",
        "\n",
        "    return chapter_keywords\n",
        "\n",
        "def generate_meaningful_titles(chapter_keywords, chapters_original):\n",
        "    chapter_titles = []\n",
        "    for i, keywords in enumerate(chapter_keywords):\n",
        "        meaningful_words = []\n",
        "        for word in keywords[:5]:\n",
        "            if len(word) > 4 and not word.endswith(('ing', 'ed', 'ly')):\n",
        "                meaningful_words.append(word)\n",
        "            if len(meaningful_words) >= 3:\n",
        "                break\n",
        "        if len(meaningful_words) < 2:\n",
        "            meaningful_words = keywords[:3]\n",
        "        title = \" \".join(meaningful_words).title()\n",
        "        chapter_titles.append(f\"Chapter {i+1}: {title}\")\n",
        "\n",
        "    return chapter_titles\n",
        "\n",
        "thematic_keywords = thematic_tfidf_analysis(chapters)\n",
        "thematic_titles = generate_meaningful_titles(thematic_keywords, chapters)\n",
        "\n",
        "for i, (keywords, title) in enumerate(zip(thematic_keywords, thematic_titles)):\n",
        "    print(f\"Chapter {i+1}: {keywords}\")\n",
        "    print(f\"Title: {title}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULOXpWpKGYUi",
        "outputId": "75ff9d8c-1f18-4bfc-e899-acab3ecf3066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1: ['door', 'rabbit', 'cat', 'mouse', 'replied', 'three', 'turtle', 'queen', 'march hare', 'march']\n",
            "Title: Chapter 1: Rabbit Mouse\n",
            "\n",
            "Chapter 2: ['mouse', 'cat', 'rabbit', 'door', 'duchess', 'replied', 'turtle', 'three', 'march hare', 'queen']\n",
            "Title: Chapter 2: Mouse Rabbit Duchess\n",
            "\n",
            "Chapter 3: ['mouse', 'course', 'replied', 'cat', 'three', 'turtle', 'rabbit', 'queen', 'march hare', 'march']\n",
            "Title: Chapter 3: Mouse Course Three\n",
            "\n",
            "Chapter 4: ['rabbit', 'door', 'duchess', 'mouse', 'caterpillar', 'three', 'replied', 'turtle', 'march hare', 'queen']\n",
            "Title: Chapter 4: Rabbit Duchess Mouse\n",
            "\n",
            "Chapter 5: ['caterpillar', 'replied', 'three', 'door', 'rabbit', 'turtle', 'queen', 'mouse', 'march hare', 'march']\n",
            "Title: Chapter 5: Caterpillar Three Rabbit\n",
            "\n",
            "Chapter 6: ['cat', 'duchess', 'door', 'march', 'march hare', 'hatter', 'hare', 'queen', 'replied', 'rabbit']\n",
            "Title: Chapter 6: Duchess March March Hare\n",
            "\n",
            "Chapter 7: ['hatter', 'dormouse', 'march hare', 'march', 'hare', 'replied', 'course', 'three', 'queen', 'door']\n",
            "Title: Chapter 7: Hatter Dormouse March Hare\n",
            "\n",
            "Chapter 8: ['queen', 'king', 'cat', 'three', 'rabbit', 'duchess', 'replied', 'turtle', 'mock', 'march hare']\n",
            "Title: Chapter 8: Queen Three Rabbit\n",
            "\n",
            "Chapter 9: ['turtle', 'mock turtle', 'mock', 'gryphon', 'duchess', 'queen', 'course', 'replied', 'king', 'three']\n",
            "Title: Chapter 9: Turtle Mock Turtle Gryphon\n",
            "\n",
            "Chapter 10: ['turtle', 'mock turtle', 'gryphon', 'mock', 'replied', 'course', 'caterpillar', 'rabbit', 'three', 'march hare']\n",
            "Title: Chapter 10: Turtle Mock Turtle Gryphon\n",
            "\n",
            "Chapter 11: ['hatter', 'king', 'dormouse', 'queen', 'rabbit', 'march hare', 'march', 'hare', 'three', 'gryphon']\n",
            "Title: Chapter 11: Hatter Dormouse Queen\n",
            "\n",
            "Chapter 12: ['king', 'queen', 'rabbit', 'turtle', 'mock turtle', 'gryphon', 'mock', 'course', 'march hare', 'march']\n",
            "Title: Chapter 12: Queen Rabbit Turtle\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TF-IDF algorithm has successfully identified the thematic structure of the work, generating meaningful chapter titles based on key characters and objects. The method demonstrated the ability to track the development of the plot through the appearance and disappearance of specific characters, but retained limitations in the form of some repetition of common terms and the mechanistic nature of the titles, which, although reflecting the substantive elements of the chapters, do not always convey their semantic depth or emotional coloring.\n",
        "\n",
        "Let's try another method - Word2vec to compare the results"
      ],
      "metadata": {
        "id": "kNKGKXe34z9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from collections import Counter\n",
        "import string\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "t0h1uvZwD208"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the function with Word2vec method to find 10 important words and generate chapters' titles"
      ],
      "metadata": {
        "id": "YNeKcfUV48ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chapter_analysis_word2vec(chapters):\n",
        "\n",
        "    chapter_sentences = []\n",
        "    for i, chapter in enumerate(chapters):\n",
        "      # Splitting the chapter into sentences\n",
        "        sentences = sent_tokenize(chapter)\n",
        "        chapter_sentence_tokens = []\n",
        "        # Tokenization and text purification\n",
        "        for sent in sentences:\n",
        "            words = word_tokenize(sent.lower())\n",
        "            words = [word for word in words\n",
        "                    if word not in stop_words\n",
        "                    and word not in string.punctuation\n",
        "                    and len(word) > 2\n",
        "                    and word not in ['alice', 'said']]\n",
        "            if words:\n",
        "                chapter_sentence_tokens.append(words)\n",
        "\n",
        "        chapter_sentences.append(chapter_sentence_tokens)\n",
        "    # Combining the data for training\n",
        "    all_sentences = []\n",
        "    for chapter in chapter_sentences:\n",
        "        all_sentences.extend(chapter)\n",
        "    # Training Word2vec model\n",
        "    model = Word2Vec(\n",
        "        sentences=all_sentences,\n",
        "        vector_size=100,\n",
        "        window=5,\n",
        "        min_count=2,\n",
        "        workers=4,\n",
        "        epochs=50\n",
        "    )\n",
        "\n",
        "    print(f\"Dict size: {len(model.wv.key_to_index)}\")\n",
        "    chapter_keywords_w2v = []\n",
        "    chapter_titles_w2v = []\n",
        "    for i, chapter in enumerate(chapter_sentences):\n",
        "        chapter_words = []\n",
        "        for sentence in chapter:\n",
        "            chapter_words.extend(sentence)\n",
        "\n",
        "        if not chapter_words:\n",
        "            print(f\"Chapter {i+1}: no matching words\")\n",
        "            chapter_keywords_w2v.append([f\"word_{j}\" for j in range(1, 11)])\n",
        "            chapter_titles_w2v.append(f\"Chapter {i+1}\")\n",
        "            continue\n",
        "        # Analisys of words frequency\n",
        "        word_freq = Counter(chapter_words)\n",
        "        common_words = [word for word, count in word_freq.most_common(50) if word in model.wv.key_to_index]\n",
        "\n",
        "        if not common_words:\n",
        "            print(f\"Chapter {i+1}: no words in model\")\n",
        "            backup_words = [word for word, count in word_freq.most_common(20) if word not in stop_words and len(word) > 2]\n",
        "            if len(backup_words) >= 10:\n",
        "                chapter_keywords_w2v.append(backup_words[:10])\n",
        "            else:\n",
        "                while len(backup_words) < 10:\n",
        "                    backup_words.extend(backup_words)\n",
        "                chapter_keywords_w2v.append(backup_words[:10])\n",
        "            chapter_titles_w2v.append(f\"Chapter {i+1}\")\n",
        "            continue\n",
        "\n",
        "        chapter_vectors = []\n",
        "        for word in common_words[:20]:\n",
        "            if word in model.wv.key_to_index:\n",
        "                chapter_vectors.append(model.wv[word])\n",
        "\n",
        "        if not chapter_vectors:\n",
        "            backup_words = [word for word, count in word_freq.most_common(10)]\n",
        "            chapter_keywords_w2v.append(backup_words)\n",
        "            chapter_titles_w2v.append(f\"Chapter {i+1}\")\n",
        "            continue\n",
        "\n",
        "        mean_vector = np.mean(chapter_vectors, axis=0)\n",
        "        try:\n",
        "            similar_words = model.wv.similar_by_vector(\n",
        "                mean_vector,\n",
        "                topn=30,\n",
        "                restrict_vocab=None\n",
        "            )\n",
        "\n",
        "            filtered_words = []\n",
        "            for word, score in similar_words:\n",
        "                if (word not in stop_words and\n",
        "                    len(word) > 2 and\n",
        "                    word not in ['said', 'like', 'would', 'could', 'one', 'went', 'go'] and\n",
        "                    word_freq.get(word, 0) >= 1):\n",
        "                    filtered_words.append((word, score, word_freq.get(word, 0)))\n",
        "\n",
        "            filtered_words.sort(key=lambda x: (x[1] * 0.7 + min(x[2]/10, 1) * 0.3), reverse=True)\n",
        "            top_keywords = [word for word, score, freq in filtered_words[:10]]\n",
        "\n",
        "            if len(top_keywords) < 10:\n",
        "                additional_words = [word for word, count in word_freq.most_common(20) if word not in top_keywords and word not in stop_words and len(word) > 2]\n",
        "\n",
        "                for word in additional_words:\n",
        "                    if len(top_keywords) >= 10:\n",
        "                        break\n",
        "                    if word not in top_keywords:\n",
        "                        top_keywords.append(word)\n",
        "            top_keywords = top_keywords[:10]\n",
        "            chapter_keywords_w2v.append(top_keywords)\n",
        "            title = \" \".join(top_keywords[:3]).title()\n",
        "            chapter_titles_w2v.append(f\"Chapter {i+1}: {title}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analisys {i+1}: {e}\")\n",
        "            backup_words = [word for word, count in word_freq.most_common(10)]\n",
        "            chapter_keywords_w2v.append(backup_words)\n",
        "            chapter_titles_w2v.append(f\"Chapter {i+1}\")\n",
        "\n",
        "    return chapter_keywords_w2v, chapter_titles_w2v, model\n",
        "\n",
        "chapter_keywords_w2v, chapter_titles_w2v, w2v_model = chapter_analysis_word2vec(chapters)\n",
        "\n",
        "for i, (keywords, title) in enumerate(zip(chapter_keywords_w2v, chapter_titles_w2v)):\n",
        "    print(f\"\\nChapter {i+1}: {title}\")\n",
        "    print(f\"Kye words: {keywords}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_wCkjizD93R",
        "outputId": "1f0e5cd4-99c8-44e8-db93-c6c5834958c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dict size: 1332\n",
            "\n",
            "Chapter 1: Chapter 1: Either Fall Marked\n",
            "Kye words: ['either', 'fall', 'marked', 'drink', 'happens', 'latitude', 'earth', 'larger', 'matter', 'occurred']\n",
            "\n",
            "Chapter 2: Chapter 2: English Remembered Stay\n",
            "Kye words: ['english', 'remembered', 'stay', 'tail', 'strange', 'besides', 'number', 'walk', 'speaking', 'directions']\n",
            "\n",
            "Chapter 3: Chapter 3: Dry Tale English\n",
            "Kye words: ['dry', 'tale', 'english', 'wanted', 'stay', 'temper', 'absurd', 'world', 'mouse', 'dodo']\n",
            "\n",
            "Chapter 4: Chapter 4: Window Noticed Bark\n",
            "Kye words: ['window', 'noticed', 'bark', 'together', 'foot', 'taking', 'effect', 'middle', 'pressed', 'face']\n",
            "\n",
            "Chapter 5: Chapter 5: Eggs Serpents _What_\n",
            "Kye words: ['eggs', 'serpents', '_what_', 'temper', 'keep', 'case', 'jaws', 'managed', 'number', 'several']\n",
            "\n",
            "Chapter 6: Chapter 6: Creature Wood Vanished\n",
            "Kye words: ['creature', 'wood', 'vanished', 'matter', 'help', 'yet.', 'nearer', 'either', 'extraordinary', 'themâ€”']\n",
            "\n",
            "Chapter 7: Chapter 7: Answer Butter Live\n",
            "Kye words: ['answer', 'butter', 'live', 'bottom', 'place', 'angrily', 'cup', 'whisper', 'wine', 'yawning']\n",
            "\n",
            "Chapter 8: Chapter 8: Hedgehog Rose-Tree Appeared\n",
            "Kye words: ['hedgehog', 'rose-tree', 'appeared', 'roses', 'silent', 'children', 'watching', 'lie', 'flat', 'eager']\n",
            "\n",
            "Chapter 9: Chapter 9: Day School Used\n",
            "Kye words: ['day', 'school', 'used', 'yes', 'taught', 'history', 'sea', 'really', 'lady', '_he_']\n",
            "\n",
            "Chapter 10: Chapter 10: Whiting Lobsters Sea\n",
            "Kye words: ['whiting', 'lobsters', 'sea', 'song', 'sing', 'far', 'different', 'quadrille', 'advance', 'explain']\n",
            "\n",
            "Chapter 11: Chapter 11: Slates Officers Bread-And-Butter\n",
            "Kye words: ['slates', 'officers', 'bread-and-butter', 'hearts', 'teacup', 'list', 'man', 'staring', 'knave', 'write']\n",
            "\n",
            "Chapter 12: Chapter 12: Read Directed Grass\n",
            "Kye words: ['read', 'directed', 'grass', 'adventures', 'seems', 'fact', 'picking', 'guests', 'top', 'children']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Word2vec method has demonstrated the ability to identify semantically rich and contextually specific words for each chapter, generating original and linguistically diverse titles. However, the algorithm showed a tendency to identify semantically vivid but plot-related terms, which led to the creation of low-content titles that poorly correlate with the main events of the chapters and key characters of the work, which indicates the limited applicability of the approach for the tasks of accurate thematic analysis of narrative texts.\n",
        "\n",
        "Finding the top 10 most used verbs in sentences with Alice.\n",
        "Using search of phrasal verbs"
      ],
      "metadata": {
        "id": "xXafZpcT7ioO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_alice_verbs_with_phrasal(chapters):\n",
        "\n",
        "    grammar = r\"\"\"\n",
        "    PHRASAL_VERB:\n",
        "        {<VB.?><RP|IN|RB>}\n",
        "    \"\"\"\n",
        "    # Creating a parser for highlighting syntactic groups\n",
        "    chunker = nltk.RegexpParser(grammar)\n",
        "    # Collecting all the sentences where Alice is mentioned\n",
        "    all_alice_sentences = []\n",
        "    for i, chapter in enumerate(chapters):\n",
        "        sentences = sent_tokenize(chapter)\n",
        "        for sent in sentences:\n",
        "            if re.search(r'\\balice\\b', sent, re.IGNORECASE):\n",
        "                all_alice_sentences.append(sent)\n",
        "\n",
        "    verbs = []\n",
        "    stop_verbs = {'was', 'is', 'were', 'be', 'have', 'has', 'had', 'do', 'does', 'did', 'are', 'am'}\n",
        "    used_indices = set()\n",
        "\n",
        "    for i, sent in enumerate(all_alice_sentences):\n",
        "        try:\n",
        "            words = word_tokenize(sent)\n",
        "            pos_tags = nltk.pos_tag(words)\n",
        "            used_indices.clear()\n",
        "            tree = chunker.parse(pos_tags)\n",
        "            #  Looking for phrasal verbs in the parsing tree\n",
        "            for subtree in tree.subtrees():\n",
        "                if subtree.label() == 'PHRASAL_VERB':\n",
        "                    phrasal_verb = ' '.join(word for word, tag in subtree.leaves()).lower()\n",
        "                    if len(phrasal_verb) > 1:\n",
        "                        verbs.append(phrasal_verb)\n",
        "                    start_index = None\n",
        "                    for j, (word, tag) in enumerate(pos_tags):\n",
        "                        if (word, tag) in subtree.leaves():\n",
        "                            if start_index is None:\n",
        "                                start_index = j\n",
        "                            used_indices.add(j)\n",
        "            # Processing single verbs that are not included in the compound verbs\n",
        "            for j, (word, tag) in enumerate(pos_tags):\n",
        "                if j in used_indices:\n",
        "                    continue\n",
        "\n",
        "                if tag.startswith('VB'):\n",
        "                    if (word.isalpha() and\n",
        "                        len(word) > 1 and\n",
        "                        word.lower() not in stop_verbs):\n",
        "\n",
        "                        lemma = lemmatizer.lemmatize(word.lower(), pos='v')\n",
        "                        if lemma.isalpha() and len(lemma) > 1:\n",
        "                            verbs.append(lemma)\n",
        "        except Exception as e:\n",
        "            if i < 10:\n",
        "                print(f\"Error in sentence {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    verb_counts = Counter(verbs)\n",
        "    top_verbs = verb_counts.most_common(15)\n",
        "\n",
        "    return top_verbs, all_alice_sentences\n",
        "\n",
        "top_verbs_with_phrasal, alice_sentences = analyze_alice_verbs_with_phrasal(chapters)\n",
        "print(\"Alice's 15 most frequent actions\")\n",
        "for i, (verb, count) in enumerate(top_verbs_with_phrasal, 1):\n",
        "    print(f\"{verb:20} {count:4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnObAW363P_K",
        "outputId": "d9797b59-73ca-4f8a-e6a6-4e53229d0d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice's 15 most frequent actions\n",
            "say                   270\n",
            "think                  65\n",
            "know                   39\n",
            "see                    34\n",
            "begin                  30\n",
            "go                     30\n",
            "get                    29\n",
            "make                   27\n",
            "don                    27\n",
            "take                   26\n",
            "went on                23\n",
            "tell                   23\n",
            "be                     21\n",
            "find                   20\n",
            "seem                   20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we cat see that the algorith found verbs which are most frequent. the list includes combined words like \"went on\", but also there are some mistakes like wod \"don\" - most likely it's the verb don't because there is the removing punctuation marks and the lemmatization is used so it is not the word \"done\".\n",
        "\n",
        "Finding the top 10 most used verbs in sentences with Alice.\n",
        "Using the Word2vec method with one central verb and a list of similar words"
      ],
      "metadata": {
        "id": "t79o9x_y8CLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_verbs_word2vec(chapters):\n",
        "\n",
        "    all_alice_sentences = []\n",
        "    for chapter in chapters:\n",
        "        sentences = sent_tokenize(chapter)\n",
        "        for sent in sentences:\n",
        "            if re.search(r'\\balice\\b', sent, re.IGNORECASE):\n",
        "                all_alice_sentences.append(sent)\n",
        "\n",
        "    processed_sentences = []\n",
        "    for sent in all_alice_sentences:\n",
        "        words = word_tokenize(sent.lower())\n",
        "        words = [word for word in words\n",
        "                if word.isalpha() and word not in stop_words and len(word) > 2]\n",
        "        if words:\n",
        "            processed_sentences.append(words)\n",
        "\n",
        "    model = Word2Vec(\n",
        "        sentences=processed_sentences,\n",
        "        vector_size=100,\n",
        "        window=5,\n",
        "        min_count=2,\n",
        "        workers=4,\n",
        "        epochs=30\n",
        "    )\n",
        "\n",
        "    verbs = []\n",
        "    stop_verbs = {'was', 'is', 'were', 'be', 'have', 'has', 'had', 'do', 'does', 'did'}\n",
        "    for sent in all_alice_sentences:\n",
        "        try:\n",
        "            words = word_tokenize(sent)\n",
        "            pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "            for word, tag in pos_tags:\n",
        "                if tag.startswith('VB'):\n",
        "                    lemma = lemmatizer.lemmatize(word.lower(), pos='v')\n",
        "                    if (lemma.isalpha() and len(lemma) > 1 and\n",
        "                        lemma not in stop_verbs and lemma in model.wv):\n",
        "                        verbs.append(lemma)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    verb_groups = cluster_verbs_word2vec(verbs, model)\n",
        "\n",
        "    return verb_groups, verbs, model\n",
        "\n",
        "# Clusterizes verbs into semantically similar groups using K-means based on Word2Vec vector representations\n",
        "def cluster_verbs_word2vec(verbs, model, n_clusters=8):\n",
        "\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    unique_verbs = list(set([v for v in verbs if v in model.wv]))\n",
        "\n",
        "    if not unique_verbs:\n",
        "        return {}\n",
        "    verb_vectors = np.array([model.wv[verb] for verb in unique_verbs])\n",
        "\n",
        "    kmeans = KMeans(n_clusters=min(n_clusters, len(unique_verbs)), random_state=42)\n",
        "    clusters = kmeans.fit_predict(verb_vectors)\n",
        "\n",
        "    verb_groups = {}\n",
        "    for verb, cluster_id in zip(unique_verbs, clusters):\n",
        "        if cluster_id not in verb_groups:\n",
        "            verb_groups[cluster_id] = []\n",
        "        verb_groups[cluster_id].append(verb)\n",
        "\n",
        "    cluster_centers = {}\n",
        "    for cluster_id in verb_groups:\n",
        "        cluster_verbs = verb_groups[cluster_id]\n",
        "        if len(cluster_verbs) > 1:\n",
        "            cluster_vector = np.mean([model.wv[verb] for verb in cluster_verbs], axis=0)\n",
        "            similarities = [cosine_similarity([model.wv[verb]], [cluster_vector])[0][0] for verb in cluster_verbs]\n",
        "            best_verb = cluster_verbs[np.argmax(similarities)]\n",
        "            cluster_centers[cluster_id] = (best_verb, cluster_verbs)\n",
        "        else:\n",
        "            cluster_centers[cluster_id] = (cluster_verbs[0], cluster_verbs)\n",
        "\n",
        "    return cluster_centers\n",
        "\n",
        "verb_groups, all_verbs, w2v_model = analyze_verbs_word2vec(chapters)\n",
        "\n",
        "for cluster_id, (center_verb, group_verbs) in verb_groups.items():\n",
        "    print(f\"\\nCentral verb: {center_verb}\")\n",
        "    print(f\" Verbs: {', '.join(group_verbs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwrmAXGcQ1lx",
        "outputId": "40c42467-3183-41e7-f916-8a16cae081dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Central verb: alarm\n",
            " Verbs: fall, upset, rush, lie, doubt, chance, hold, hurt, repeat, notice, ashamed, introduce, sound, twelve, drink, treacle, alarm, stay\n",
            "\n",
            "Central verb: finish\n",
            " Verbs: appear, answer, queer, particular, give, stand, reply, explain, fear, manage, happen, box, surprise, sentence, finish, learn, run\n",
            "\n",
            "Central verb: want\n",
            " Verbs: understand, wonder, sir, live, want, feel, speak, draw, remember, saw, begin, change, talk, call, hear, fan, suppose, close, sleep, keep, rest\n",
            "\n",
            "Central verb: twice\n",
            " Verbs: walk, thoroughly, breathe, listen, beg, become, yer, twice, dare, play, help, beat, read, turn, set, dance, escape, fell\n",
            "\n",
            "Central verb: face\n",
            " Verbs: grow, use, get, say, make, hurry, leave, face, afraid, dear, see, please, felt, think, look, tell, watch, put\n",
            "\n",
            "Central verb: matter\n",
            " Verbs: move, matter, dream, seem, try, grin, mean, spread, lay, croquet, ask, eat, savage, believe, remark, dry, mind, allow, wish, care\n",
            "\n",
            "Central verb: find\n",
            " Verbs: round, next, let, long, find, hand, know, queen, take, last, end, till, like, come, alice\n",
            "\n",
            "Central verb: agree\n",
            " Verbs: sigh, wink, stop, fly, agree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An analysis of the verb clusterization results using the Word2vec and k-means method revealed significant limitations of this approach for semantic text analysis. The algorithm demonstrated a low clustering quality, expressed in the semantic heterogeneity of the groups. The problem is compounded by the incorrect definition of central words, where rare terms such as \"alarm\" or the adverb \"twice\" do not reflect the semantic core of their clusters. The method also failed to filter parts of speech by including non-verbs in groups, which indicates the disadvantages of data preprocessing. The results obtained demonstrate the fundamental limitations of the method when working with small text data rather than providing meaningful linguistic information.\n"
      ],
      "metadata": {
        "id": "lQ7rQk7xEgCJ"
      }
    }
  ]
}